## web crawler
readme-edit

run Second.sh to save html keyword page

soup_parse.py <-- get name and link to profile page [separate personal page from business page already] this export different csv 

save_page <-- where the saved raw html and csv data saved



###Now working on 
-read_csv.py to use data from csv to get basic contact info



###Need to concern
-Name of the source code should relate to what it really does
-Location of folder
-Delete every file that are not needed

